{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grd\n",
    "from evaluation_functions import ensemble_evaluation_functions as eef\n",
    "import sunspots.sunspots as sunspots\n",
    "\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "import huxt as H\n",
    "import huxt_analysis as HA\n",
    "import huxt_inputs as Hin\n",
    "\n",
    "import scipy.interpolate\n",
    "from scipy import integrate\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from sunpy.coordinates.sun import carrington_rotation_time\n",
    "from sunpy.coordinates.sun import carrington_rotation_number\n",
    "import astropy.units as u\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_deterministic_forecast_cdf(cr):\n",
    "    ### Reads in deterministic forecast runs\n",
    "    input_file = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_forecasting\\\\data\\\\deterministic_forecasts\\\\deterministic_CR{cr}.nc'\n",
    "\n",
    "    # Read the NetCDF file into an xarray Dataset\n",
    "    loaded_ds = xr.open_dataset(input_file)\n",
    "\n",
    "    df = loaded_ds.sel().to_dataframe()\n",
    "\n",
    "    return df\n",
    "\n",
    "def compute_roc_score(roc_curve):\n",
    "    \"\"\"\n",
    "    Computes integrates area under ROC curve using scipy quad returning integrated area as the ROC score\n",
    "\n",
    "    Parameters:\n",
    "    - roc_curve (list): list of tuples which each contain the hit rate and false alarm rate calculated at different thresholds\n",
    "\n",
    "    Returns:\n",
    "    - result (float): ROC Score calculated as the integrated area under ROC curve\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack and prepare roc curve data\n",
    "    y,x = zip(*roc_curve)\n",
    "\n",
    "    x = np.array([xx for xx in x])\n",
    "    y = np.array([yy for yy in y])\n",
    "\n",
    "    nan_mask_x = ~np.isnan(x)\n",
    "    nan_mask_y = ~np.isnan(y)\n",
    "\n",
    "    nan_mask = np.logical_and(nan_mask_x, nan_mask_y)\n",
    "\n",
    "    x = x[nan_mask]\n",
    "    y = y[nan_mask]\n",
    "\n",
    "    # Interpolate the curve\n",
    "    interp_function = scipy.interpolate.interp1d(x, y, kind='linear')\n",
    "\n",
    "    # Define the integration limits\n",
    "    a = min(x)\n",
    "    b = max(x)\n",
    "\n",
    "    # Perform the integration\n",
    "    result, error = integrate.quad(interp_function, a, b)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def interpolate_and_resample(observed_data_index, forecast_index, forecast):\n",
    "    \n",
    "    # This function runs an interpolation algorithm on forecast output and outputs a resamples forecast series on the omni data timestep\n",
    "    Int = scipy.interpolate.CubicSpline(forecast_index, forecast)\n",
    "\n",
    "    interpolated_forecast_output = Int(observed_data_index)\n",
    "\n",
    "    return interpolated_forecast_output\n",
    "\n",
    "\n",
    "def perturb_longitude(long_pert_dt, ensemble_member):\n",
    "    \n",
    "    # Ensure that the DataFrame has a datetime index\n",
    "    if not isinstance(ensemble_member.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame must have a datetime index\")\n",
    "    \n",
    "    shift_amount = int(long_pert_dt.to(u.second).value) # Convert shift amount into seconds\n",
    "\n",
    "    # Convert datetime index into integers\n",
    "    numeric_index = ensemble_member.index.astype(int) // 10**9 # Floor division to convert into seconds (UNIX int datetime is in nanoseconds)\n",
    "    numeric_index = numeric_index.to_numpy()\n",
    "\n",
    "    # Calculate the new index by shifting with wrapping\n",
    "    shifted_index = numeric_index + shift_amount\n",
    "\n",
    "    # Generate boolean array which captures elements above and below max/min indices\n",
    "    wrap_mask = (shifted_index < numeric_index.min()) | (shifted_index > numeric_index.max())\n",
    "  \n",
    "    if shift_amount < 0:\n",
    "        # If negative shift amount (i.e. elements will be below min index)\n",
    "        shifted_index[wrap_mask] = shifted_index[wrap_mask] + (shifted_index.max() - shifted_index.min())\n",
    "\n",
    "    elif shift_amount > 0:\n",
    "        # If positive shift amount (i.e. elements will be above max index)  \n",
    "        shifted_index[wrap_mask] = shifted_index[wrap_mask] - (shifted_index.max() - shifted_index.min())\n",
    "\n",
    "    else:\n",
    "        # No change -- preserve original index\n",
    "        shifted_index = numeric_index\n",
    "\n",
    "    # Convert back to datetime index\n",
    "    new_converted_index = pd.to_datetime(shifted_index, unit='s')\n",
    "\n",
    "    # Sort the DataFrame based on the new index\n",
    "    df_shifted = ensemble_member.copy()\n",
    "    df_shifted = (df_shifted.sort_index().set_index(new_converted_index)).sort_index()\n",
    "    df_shifted = df_shifted[~(df_shifted.index).duplicated()].copy()\n",
    "\n",
    "    # Interpolate shifted dataframe back onto orignal datetime axis (for more ease in later analysis)\n",
    "    Int = scipy.interpolate.CubicSpline(df_shifted.index, df_shifted['vsw'])\n",
    "    data_time_axis = ensemble_member.index\n",
    "    interpolated_forecast_output = Int(data_time_axis)\n",
    "    \n",
    "    return pd.DataFrame({'vsw':interpolated_forecast_output}, index = data_time_axis)\n",
    "\n",
    "\n",
    "def gen_ranked_ensemble(ensemble_members, omni_data): \n",
    "\n",
    "    vsw_list = [] # Initialise list for forecast output\n",
    "\n",
    "    # Prepare data for rank comparison\n",
    "    omni_chunk = omni_data.loc[pd.to_datetime(ensemble_members[0].index[0]):pd.to_datetime(ensemble_members[0].index[-1])]\n",
    " \n",
    "    omni_chunk = omni_chunk.dropna(subset = ['V']) # Remove rows with NaN values\n",
    "\n",
    "    # Interpolate and resample forecast output onto OMNI data time step\n",
    "    for vsw in ensemble_members:\n",
    "        vsw_int = interpolate_and_resample(observed_data_index = omni_chunk.index, forecast_index=vsw.index, forecast=vsw['vsw'])\n",
    "        vsw_list.append(vsw_int)\n",
    "\n",
    "    # Compare ensemble member output arrays to omni data \n",
    "    vsw_arr = np.array(vsw_list)\n",
    "    ranked_forecast_boolean = np.array([vsw < omni_chunk['V'] for vsw in vsw_arr])\n",
    "    summed_ranks = np.sum(ranked_forecast_boolean, axis = 0)\n",
    "    \n",
    "    return summed_ranks\n",
    "\n",
    "def calculate_rank_chi_square(ensemble_size, ranked_forecasts):\n",
    "        \n",
    "    ensemble_hist = np.histogram(ranked_forecasts, bins = ensemble_size)[0]\n",
    "\n",
    "    chi_sq_alt = 0 \n",
    "    \n",
    "    for i in range(ensemble_size):\n",
    "        \n",
    "        chi_sq_alt += (ensemble_hist[i]/np.sum(ensemble_hist) - 1/(ensemble_size+1))**2\n",
    "\n",
    "    chi_sq_alt = chi_sq_alt*np.sum(ensemble_hist)*(ensemble_size+1)\n",
    "\n",
    "    return chi_sq_alt\n",
    "\n",
    "def chi_rank_over_longitude_range(ensemble_sets, long_min, long_max, increments, ensemble_size):\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    carrington_rotation = 27.2753*u.day\n",
    "    long_pert_var_list = np.linspace(long_min, long_max, increments) # Define range of longitude variances to perturb by\n",
    "    chi_square_across_longitude = np.ones(increments)\n",
    "\n",
    "    # Loop through each CR set and longitudinally peturb each ensemble member whilst retaining ensemble set structure\n",
    "    for i, long_pert_var in enumerate(long_pert_var_list):\n",
    "        \n",
    "        lp_ensemble_sets = [] # list to store sets of longitudinally perturbed ensemble members per CR\n",
    "        ranked_ensemble_sets = [] # list of ranked ensemble sets per CR\n",
    "\n",
    "        for ensemble_members in ensemble_sets:\n",
    "\n",
    "            lp_ensemble_members = [] # list to store longitudinally perturbed ensemble members\n",
    "\n",
    "            # Loop through each ensemble member to perturb in longitude\n",
    "            for df in ensemble_members:\n",
    "                \n",
    "                # Randomly generate perturbation amount\n",
    "                long_pert = rng.normal(loc = 0, scale = long_pert_var, size = 1)\n",
    "                long_pert_dt = ((long_pert * carrington_rotation.value) / 360) * u.day # Convert perturbation amount to a unit of days\n",
    "\n",
    "                # perturb ensemble member \n",
    "                shifted_df = perturb_longitude(long_pert_dt=long_pert_dt, ensemble_member=df)\n",
    "                lp_ensemble_members.append(shifted_df)\n",
    "            \n",
    "            # rank ensemble output against data\n",
    "            ranked_ensemble_sets.append(gen_ranked_ensemble(ensemble_members=lp_ensemble_members, omni_data = omni_data))\n",
    "            lp_ensemble_sets.append(lp_ensemble_members)\n",
    "        \n",
    "        # calculate chi squared for combined rank histograms for time frame\n",
    "        rank_chi_square = calculate_rank_chi_square(ensemble_size=ensemble_size, ranked_forecasts=np.concatenate(ranked_ensemble_sets))\n",
    "        chi_square_across_longitude[i] = rank_chi_square\n",
    "\n",
    "    return chi_square_across_longitude\n",
    "\n",
    "def save_chi_arr_to_file(chi_set_list, era_key):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\rank_analysis\\\\rank_hist_{era_key}.csv'\n",
    "    np.savetxt(fname = fname, X = chi_set_list, delimiter = ',')\n",
    "    return\n",
    "\n",
    "def read_in_and_perturb_ensemble(CR_chunk, lat_dev, long_pert_var, omni_data, ensemble_size):\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    carrington_rotation = 27.2753*u.day\n",
    "\n",
    "    ensemble_sets = [] # List to store each CR ensemble set \n",
    "    data_chunks = [] # List to store data chunk associated with each CR\n",
    "\n",
    "    # Read in ensemble members per CR\n",
    "    for CR in CR_chunk:\n",
    "\n",
    "        ensemble_members = eef.read_ens_cdf_var(cr=CR, var_dev = lat_dev, no_members=ensemble_size)\n",
    "        ensemble_members = [df.set_index('datetime') for df in ensemble_members]\n",
    "        ensemble_sets.append(ensemble_members)\n",
    "\n",
    "        CR_start = pd.Timestamp(carrington_rotation_time(CR).to_datetime()).round('60min')\n",
    "        CR_end = CR_start + pd.Timedelta(carrington_rotation.value, unit = 'days')\n",
    "        data_chunks.append(omni_data.loc[CR_start:CR_end])\n",
    "\n",
    "    # combining data frame will be useful later on\n",
    "    combined_data = pd.concat(data_chunks)\n",
    "\n",
    "    lp_ensemble_sets = [] # list to store sets of longitudinally perturbed ensemble members per CR\n",
    "\n",
    "    # Loop through each CR set and longitudinally peturb each ensemble member whilst retaining ensemble set structure\n",
    "    for i, ensemble_members in enumerate(ensemble_sets):\n",
    "\n",
    "        lp_ensemble_members = []\n",
    "\n",
    "        for df in ensemble_members:\n",
    "\n",
    "            # Perturb ensemble member in longitude\n",
    "            long_pert = rng.normal(loc = 0, scale = long_pert_var, size = 1)\n",
    "            long_pert_dt = ((long_pert * carrington_rotation.value) / 360) * u.day\n",
    "            shifted_df = perturb_longitude(long_pert_dt=long_pert_dt, ensemble_member=df)\n",
    "\n",
    "            # resample perturbed ensemble member onto omni data timestep\n",
    "            Shifted_Int = scipy.interpolate.CubicSpline(shifted_df.index, shifted_df['vsw'])\n",
    "            resampled_shifted_df = pd.DataFrame({'vsw':Shifted_Int(data_chunks[i].index)}, index = data_chunks[i].index)\n",
    "            lp_ensemble_members.append(resampled_shifted_df)\n",
    "        \n",
    "        lp_ensemble_sets.append(lp_ensemble_members)\n",
    "\n",
    "    ## Longitudinally perturbed ensemble members\n",
    "    combined_lp_ensemble_members = []\n",
    "\n",
    "    # Create a new list of lists\n",
    "    for i in range(ensemble_size):\n",
    "        list_of_cr_members = [sublist[i]['vsw'] for sublist in lp_ensemble_sets]\n",
    "        combined_lp_ensemble_members.append(pd.concat(list_of_cr_members))\n",
    "\n",
    "    return combined_lp_ensemble_members, combined_data\n",
    "\n",
    "def get_data_chunks_per_CR(CR_chunk, omni_data):\n",
    "\n",
    "    carrington_rotation = 27.2753*u.day\n",
    "\n",
    "    data_chunks = [] # List to store data chunk associated with each CR\n",
    "\n",
    "    # Read in ensemble members per CR\n",
    "    for CR in CR_chunk:\n",
    "    \n",
    "        CR_start = pd.Timestamp(carrington_rotation_time(CR).to_datetime()).round('60min')\n",
    "        CR_end = CR_start + pd.Timedelta(carrington_rotation.value, unit = 'days')\n",
    "        data_chunks.append(omni_data.loc[CR_start:CR_end])\n",
    "\n",
    "    return data_chunks\n",
    "\n",
    "def compute_calibration_curve(ensemble_members, observed_data, event_threshold, num_bins):\n",
    "    \n",
    "    # generate probabiistic forecast from ensemble set\n",
    "    probabilistic_forecast = eef.gen_probabilistic_forecast(ensemble_members=ensemble_members, \n",
    "                                                            threshold=event_threshold, ensemble_size=len(ensemble_members))\n",
    "    \n",
    "    y_prob = probabilistic_forecast\n",
    "    # binarised data by event threshold\n",
    "    y_true = eef.generate_catagorical_forecast(forecast=observed_data, threshold=event_threshold)\n",
    "\n",
    "    # generate calibration curve using scikitlearn fuction - 'quantile' qwarg means bins contain equal data points\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=num_bins, strategy = 'uniform')\n",
    "\n",
    "    return (prob_true, prob_pred)\n",
    "\n",
    "def save_brier_scores_to_file(brier_scores, era_key, event_threshold):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\forecast_evaluation\\\\brier_scores_{era_key}_{event_threshold}.csv'\n",
    "    np.savetxt(fname = fname, X = brier_scores, delimiter = ',')\n",
    "    return\n",
    "\n",
    "def save_roc_scores_to_file(roc_scores, era_key, prob_thresh):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\forecast_evaluation\\\\roc_scores_{era_key}_{int(prob_thresh*10)}.csv'\n",
    "    np.savetxt(fname = fname, X = roc_scores, delimiter = ',')\n",
    "    return\n",
    "\n",
    "def save_calibration_curves_to_file(calibrations_grid, era_key, event_threshold):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\forecast_evaluation\\\\calibration_curves_{era_key}_{event_threshold}.csv'\n",
    "    np.savetxt(fname = fname, X = calibrations_grid, delimiter = ',')\n",
    "    return\n",
    "\n",
    "def read_brier_scores_file(era_key, event_thresh):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\forecast_evaluation\\\\brier_scores_{era_key}_{event_thresh}.csv'\n",
    "    return np.genfromtxt(fname = fname, delimiter=',')\n",
    "\n",
    "def read_rank_hist_file(era_key):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\rank_analysis\\\\rank_hist_{era_key}.csv'\n",
    "    return np.genfromtxt(fname = fname, delimiter=',')\n",
    "\n",
    "def read_ROC_scores_file(era_key, prob_thresh):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\forecast_evaluation\\\\roc_scores_{era_key}_{int(prob_thresh*10)}.csv'\n",
    "    return np.genfromtxt(fname = fname, delimiter=',')\n",
    "\n",
    "def read_calibration_curves_file(era_key, event_thresh):\n",
    "    fname = f'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\forecast_evaluation\\\\calibration_curves_{era_key}_{event_thresh}.csv'\n",
    "    return np.genfromtxt(fname = fname, delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in and prepare OMNI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omni data directory\n",
    "omni_data_dir = 'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\OMNI\\\\Processed_omni\\\\'\n",
    "\n",
    "# load the data into dataframe and index by datetime\n",
    "omni_data = pd.read_hdf(omni_data_dir + 'omni_1hour.h5')\n",
    "omni_data = omni_data.set_index('datetime')\n",
    "omni_data = omni_data.dropna(subset = ['V']) # Remove rows with NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perturbation scheme impact on timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_dev = 15\n",
    "long_dev = 0\n",
    "ensemble_size = 100\n",
    "CR_chunk = [2194]\n",
    "ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "lp_lat_dev = 15\n",
    "lp_long_dev = 10\n",
    "lp_ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lp_lat_dev, long_pert_var=lp_long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "max_speed = np.percentile(ensemble_members, 34.1, axis = 0)\n",
    "min_speed = np.percentile(ensemble_members, 100-34.1, axis = 0)\n",
    "\n",
    "lp_max_speed = np.percentile(lp_ensemble_members, 34.1, axis = 0)\n",
    "lp_min_speed = np.percentile(lp_ensemble_members, 100-34.1, axis = 0)\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 1, figsize = (13,7), dpi = 300, sharex=True)\n",
    "\n",
    "axes[0].fill_between(ensemble_members[0].index, y1 = min_speed, y2 = max_speed, label = r'1$\\sigma$ Spread', color = 'red', alpha = 0.5, lw = 0)\n",
    "axes[0].plot(ensemble_members[0].index, np.median(ensemble_members,axis=0), color = 'darkred', lw = 1, label = 'Ensemble Median')\n",
    "axes[0].set_title(f'HUXt Ensemble ($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({lat_dev}, {long_dev})')\n",
    "axes[0].set_ylabel(r'$\\mathrm{V_{sw}}$ [km/s]')\n",
    "axes[0].legend(frameon = False, fontsize = 'small')\n",
    "\n",
    "axes[1].fill_between(lp_ensemble_members[0].index, y1 = lp_min_speed, y2 = lp_max_speed, label = '1$\\sigma$ Spread', color = 'red', alpha = 0.5, lw = 0)\n",
    "axes[1].plot(lp_ensemble_members[0].index, np.median(lp_ensemble_members,axis=0), color = 'darkred', lw = 1, label = 'Ensemble Median')\n",
    "axes[1].set_title(f'HUXt Ensemble ($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({lp_lat_dev}, {lp_long_dev})')\n",
    "axes[1].set_xlabel('Time [UTC]')\n",
    "axes[1].set_ylabel(r'$\\mathrm{V_{sw}}$ [km/s]')\n",
    "#axes[1].set_xticklabels(, rotation = 45)\n",
    "axes[1].legend(frameon = False, fontsize = 'small')\n",
    "\n",
    "fname = 'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\perturbation_impact.svg'\n",
    "#plt.savefig(fname, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Carrington rotations of Solar min, Solar max, and Solar intermediate for solar cycle 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sunspot number data\n",
    "filepath = 'C:\\\\Users\\\\ct832900\\\\Desktop\\\\Research_Code\\\\Ensemble_Calibration\\\\data\\\\SN_m_tot_V2.0.csv'\n",
    "ssn = sunspots.LoadSSN(filepath = filepath)\n",
    "\n",
    "# Start and end date of solar cycle 24\n",
    "solar_start = datetime.date.fromisoformat('2008-12-01')\n",
    "solar_end = datetime.date.fromisoformat('2020-01-01')\n",
    "\n",
    "# Isolate solar cycle 24 in sunspot number data\n",
    "solar_cycle_24 = ssn.set_index('datetime')\n",
    "solar_cycle_24 = solar_cycle_24[solar_start:solar_end]\n",
    "\n",
    "# isolate solar min/max/intermediate values by solar activity index\n",
    "solar_min_mask = solar_cycle_24['sai'] < 0.33\n",
    "solar_min_mask = solar_min_mask[solar_min_mask!=0]\n",
    "\n",
    "solar_max_mask = solar_cycle_24['sai']  > 0.66\n",
    "solar_max_mask = solar_max_mask[solar_max_mask!=0]\n",
    "\n",
    "solar_other_mask = (solar_cycle_24['sai'] >= 0.33) & (solar_cycle_24['sai'] <= 0.66)\n",
    "solar_other_mask = solar_other_mask[solar_other_mask!=0]\n",
    "\n",
    "# get carrington rotations of each section of solar cycle\n",
    "solar_min_CR_list = np.unique(np.round(carrington_rotation_number(solar_min_mask.index),0).astype(int))\n",
    "solar_max_CR_list = np.unique(np.round(carrington_rotation_number(solar_max_mask.index),0).astype(int))\n",
    "solar_other_CR_list = np.unique(np.round(carrington_rotation_number(solar_other_mask.index),0).astype(int))\n",
    "\n",
    "# create dictionary for use later\n",
    "solar_era_dict = {'min':solar_min_CR_list, 'other':solar_other_CR_list, 'max':solar_max_CR_list}\n",
    "CR_chunk_key = ['min', 'other', 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(carrington_rotation_number(solar_min_mask.index))\n",
    "carrington_rotation_number(solar_min_mask.index).astype(int)\n",
    "np.round(carrington_rotation_number(solar_min_mask.index),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(solar_cycle_24['sai'], label = 'Solar Cycle 24', color = 'blue')\n",
    "plt.axhline(0.33, lw = 1, linestyle = '--', color = 'black', label = 'Activity Thresholds')\n",
    "plt.axhline(0.66, lw = 1, linestyle = '--', color = 'black')\n",
    "plt.legend(frameon = False, loc = 'upper right', fontsize = 'small')\n",
    "plt.xlabel('Time [UTC]')\n",
    "plt.ylabel('Solar Activity Index')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size = 100\n",
    "\n",
    "for key in CR_chunk_key:  \n",
    "\n",
    "    # Latitude perturbation parameters\n",
    "    lat_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "\n",
    "    CR_chunk = solar_era_dict[key] # List of CR associated with given solar era\n",
    "\n",
    "    chi_set_list = []\n",
    "\n",
    "    for lat_dev in lat_dev_list:\n",
    "\n",
    "        ensemble_sets = [] # List to store each CR ensemble set \n",
    "\n",
    "        # Read in ensemble members per CR\n",
    "        for CR in CR_chunk:\n",
    "            ensemble_members = eef.read_ens_cdf_var(cr=CR, var_dev = lat_dev, no_members=ensemble_size)\n",
    "            ensemble_members = [df.set_index('datetime') for df in ensemble_members]\n",
    "            ensemble_sets.append(ensemble_members)\n",
    "\n",
    "        chi_list = chi_rank_over_longitude_range(ensemble_sets=ensemble_sets, long_min = min(lat_dev_list), \n",
    "                                                 long_max=max(lat_dev_list), increments=len(lat_dev_list), ensemble_size=ensemble_size)\n",
    "        chi_set_list.append(chi_list)\n",
    "\n",
    "    save_chi_arr_to_file(chi_set_list=chi_set_list, era_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_histogram_grids = [read_rank_hist_file(key) for key in CR_chunk_key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(ncols = 3, nrows = 1, figsize = (10,10), sharex = True, sharey = True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "lat_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "long_dev_list = np.linspace(0,40,len(lat_dev_list))\n",
    "\n",
    "for i, key in enumerate(CR_chunk_key):\n",
    "\n",
    "    rank_hist = rank_histogram_grids[i]\n",
    "\n",
    "    im = axes[i].imshow(np.log10(rank_hist), cmap='inferno', origin = 'lower', vmin = 3.5, vmax = 6.5) \n",
    "\n",
    "    grid_range = np.max(np.log10(rank_hist)) - np.min(np.log10(rank_hist))\n",
    "    within_5percent_of_min = np.log10(rank_hist) < np.min(np.log10(rank_hist)) + grid_range*0.05\n",
    "    mindex = np.argwhere(np.log10(rank_hist)== np.min(np.log10(rank_hist)))[0]\n",
    "    axes[i].plot(mindex[1], mindex[0], marker = 'o', markersize = 5, color = 'white')\n",
    "\n",
    "    contours = axes[i].contour(within_5percent_of_min, levels = [0.5], colors='white', corner_mask = False, linewidths=1, label = r'$\\mathrm{10^th percentile range}$')\n",
    "    axes[i].set_title(f'Solar {key}', fontsize = 'small')\n",
    "\n",
    "\n",
    "axes[0].set_yticks(np.arange(len(lat_dev_list))[1::3])\n",
    "axes[0].set_ylabel(r'$\\mathrm{\\sigma_{latitude}}$ [Deg]', fontsize = 'medium')\n",
    "axes[0].set_yticklabels(np.array([str(x) for x in lat_dev_list])[1::3])\n",
    "\n",
    "outer_x_index = [0,1,2]\n",
    "for x in outer_x_index:\n",
    "    axes[x].set_xticks(np.arange(len(long_dev_list))[1::3])\n",
    "    axes[x].set_xlabel(r'$\\mathrm{\\sigma_{longitude}}$ [Deg]', fontsize = 'medium')\n",
    "    axes[x].set_xticklabels(np.array([f'{x:.0f}' for x in long_dev_list])[1::3], rotation = 0)\n",
    "\n",
    "# Create a single colorbar for all subplots\n",
    "cbar = fig.colorbar(im, ax=axes, pad=0.1, orientation = 'horizontal', aspect = 30) #, anchor = (-2,-2), aspect = 30)  \n",
    "cbar.set_label(f'$log_{{10}}(\\chi^2)$')\n",
    "\n",
    "#plt.tight_layout()\n",
    "\n",
    "fname = 'ChiSquareGrids_MMO.svg'\n",
    "path = f'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\{fname}'\n",
    "plt.savefig(path, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the calibration curve of ensembles with best/worse distributed rank histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enesmble data parameters\n",
    "key = 'min'\n",
    "lat_dev = 20\n",
    "long_dev = 10\n",
    "ensemble_size = 100\n",
    "event_threshold = 500\n",
    "\n",
    "# Compute calibration curve for best parameters at solar min\n",
    "CR_chunk = solar_era_dict[key]\n",
    "ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "cal_curve = compute_calibration_curve(ensemble_members=ensemble_members, observed_data=observed_data['V'], event_threshold=event_threshold, num_bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enesmble data parameters\n",
    "key = 'min'\n",
    "lat_dev = 5\n",
    "long_dev = 5\n",
    "ensemble_size = 100\n",
    "event_threshold = 500\n",
    "\n",
    "# Compute calibration curve for best parameters at solar min\n",
    "CR_chunk = solar_era_dict[key]\n",
    "ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "cal_curve_low_pert = compute_calibration_curve(ensemble_members=ensemble_members, observed_data=observed_data['V'], event_threshold=event_threshold, num_bins = 10)\n",
    "\n",
    "# Enesmble data parameters\n",
    "key = 'min'\n",
    "lat_dev = 40\n",
    "long_dev = 40\n",
    "ensemble_size = 100\n",
    "event_threshold = 500\n",
    "\n",
    "# Compute calibration curve for best parameters at solar min\n",
    "CR_chunk = solar_era_dict[key]\n",
    "ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "cal_curve_high_pert = compute_calibration_curve(ensemble_members=ensemble_members, observed_data=observed_data['V'], event_threshold=event_threshold, num_bins = 10)\n",
    "\n",
    "# Enesmble data parameters\n",
    "key = 'min'\n",
    "lat_dev = 20\n",
    "long_dev = 0\n",
    "ensemble_size = 100\n",
    "event_threshold = 500\n",
    "\n",
    "# Compute calibration curve for best parameters at solar min\n",
    "CR_chunk = solar_era_dict[key]\n",
    "ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "cal_curve_no_pert = compute_calibration_curve(ensemble_members=ensemble_members, observed_data=observed_data['V'], event_threshold=event_threshold, num_bins = 10)\n",
    "\n",
    "\n",
    "# Enesmble data parameters\n",
    "key = 'min'\n",
    "lat_dev = 0\n",
    "long_dev = 20\n",
    "ensemble_size = 100\n",
    "event_threshold = 500\n",
    "\n",
    "# Compute calibration curve for best parameters at solar min\n",
    "CR_chunk = solar_era_dict[key]\n",
    "ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "cal_curve_no_lat_pert = compute_calibration_curve(ensemble_members=ensemble_members, observed_data=observed_data['V'], event_threshold=event_threshold, num_bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(np.linspace(0,1,2), np.linspace(0,1,2), lw = 1, linestyle = '--', color = 'black')\n",
    "\n",
    "plt.plot(cal_curve[1], cal_curve[0], color = 'black', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({20}, {10})')\n",
    "\n",
    "plt.plot(cal_curve_high_pert[1], cal_curve_high_pert[0], color = 'blue', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({40}, {40})')\n",
    "\n",
    "plt.plot(cal_curve_low_pert[1], cal_curve_low_pert[0], color = 'red', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({5}, {5})')\n",
    "\n",
    "plt.plot(cal_curve_no_pert[1], cal_curve_no_pert[0], color = 'purple', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({20}, {0})')\n",
    "\n",
    "\n",
    "plt.plot(cal_curve_no_lat_pert[1], cal_curve_no_lat_pert[0], color = 'orange', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({0}, {20})')\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Percentage Correct')\n",
    "plt.xlabel('Forecast Probability')\n",
    "plt.title(f'Calibration Curve | Solar min')\n",
    "plt.legend(frameon = False, fontsize = 'small')\n",
    "\n",
    "fname = 'Calibration_Curves.svg'\n",
    "path = f'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\{fname}'\n",
    "#plt.savefig(path, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'min'\n",
    "\n",
    "long_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "lat_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "\n",
    "event_threshold = 500\n",
    "ensemble_size = 100\n",
    "\n",
    "briers_grid = []\n",
    "ROC_grid = []\n",
    "\n",
    "calibration_curves_grid = []\n",
    "\n",
    "CR_chunk = solar_era_dict[key]\n",
    "\n",
    "for lat_dev in lat_dev_list:\n",
    "\n",
    "    cal_curves_across_long = []\n",
    "    for long_dev in long_dev_list:\n",
    "\n",
    "        ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                    omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n",
    "        cal_curve = compute_calibration_curve(ensemble_members=ensemble_members, observed_data=observed_data['V'], event_threshold=event_threshold, num_bins = 10)\n",
    "        cal_curves_across_long.append(cal_curve)\n",
    "\n",
    "    calibration_curves_grid.append(cal_curves_across_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_freq, pred_freq = zip(*[(tup[0], tup[1]) for sublist in calibration_curves_grid for tup in sublist])\n",
    "\n",
    "obs_freq_flat = [item for sublist in obs_freq for item in sublist]\n",
    "pred_freq_flat = [item for sublist in pred_freq for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist2d(pred_freq_flat, obs_freq_flat, bins = 10, cmap = 'Greys', alpha = 0.7, label = 'All ensembles')\n",
    "\n",
    "plt.plot(np.linspace(0,1,2), np.linspace(0,1,2), lw = 1, linestyle = '--', color = 'black')\n",
    "\n",
    "plt.plot(calibration_curves_grid[4][3][1], calibration_curves_grid[4][3][0], marker = 'x', color = 'red', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({lat_dev_list[4]}, {long_dev_list[3]})')\n",
    "\n",
    "plt.plot(calibration_curves_grid[1][1][1], calibration_curves_grid[1][1][0], marker = 'x', color = 'darkblue', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({lat_dev_list[1]}, {long_dev_list[1]})')\n",
    "\n",
    "plt.plot(calibration_curves_grid[8][8][1], calibration_curves_grid[8][8][0], marker = 'x', color = 'purple', lw = 1,\n",
    "         label = f'($\\mathrm{{\\sigma_{{lat}},~\\sigma_{{lon}}}}$) = ({lat_dev_list[8]}, {long_dev_list[8]})')\n",
    "\n",
    "\n",
    "plt.colorbar()\n",
    "plt.ylabel('Observed Frequency')\n",
    "plt.xlabel('Forecast Probability')\n",
    "plt.title(f'Calibration Curve | Solar {key}')\n",
    "plt.legend(frameon = False, fontsize = 'small')\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Briers score and ROC score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'min'\n",
    "lat_dev = 15\n",
    "long_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "event_threshold = 500\n",
    "probability_threshold = 0.5\n",
    "ensemble_size = 100\n",
    "\n",
    "briers_scores_list = []\n",
    "ROC_scores_list = []\n",
    "\n",
    "for long_dev in long_dev_list:\n",
    "\n",
    "    CR_chunk = solar_era_dict[key]\n",
    "    ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                   omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "    \n",
    "    briers_scores_list.append(eef.compute_brier_score_probabilistic(ensemble_members=ensemble_members, observed_data=observed_data['V'], \n",
    "                                                                                    threshold=event_threshold, ensemble_size=ensemble_size))\n",
    "        \n",
    "    roc_curve = eef.generate_roc_curve_from_ensemble(ensemble_members=ensemble_members, observed_data=observed_data['V'], \n",
    "                                                    threshold_range=(200,800), threshold_num=30, probability_threshold = probability_threshold)\n",
    "\n",
    "    ROC_scores_list.append(compute_roc_score(roc_curve=roc_curve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'min'\n",
    "\n",
    "long_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "lat_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "\n",
    "event_threshold = 500\n",
    "probability_threshold = 0.5\n",
    "ensemble_size = 100\n",
    "\n",
    "briers_grid = []\n",
    "ROC_grid = []\n",
    "\n",
    "for lat_dev in lat_dev_list:\n",
    "\n",
    "    #initialise lists to store briers score + roc score across sigma_long\n",
    "    temp_briers_scores_list = []\n",
    "    temp_ROC_scores_list = []\n",
    "\n",
    "    for long_dev in long_dev_list:\n",
    "\n",
    "        CR_chunk = solar_era_dict[key]\n",
    "        ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                    omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "        \n",
    "        temp_briers_scores_list.append(eef.compute_brier_score_probabilistic(ensemble_members=ensemble_members, observed_data=observed_data['V'], \n",
    "                                                                                        threshold=event_threshold, ensemble_size=ensemble_size))\n",
    "            \n",
    "        roc_curve = eef.generate_roc_curve_from_ensemble(ensemble_members=ensemble_members, observed_data=observed_data['V'], \n",
    "                                                        threshold_range=(200,800), threshold_num=30, probability_threshold = probability_threshold)\n",
    "\n",
    "        temp_ROC_scores_list.append(compute_roc_score(roc_curve=roc_curve))\n",
    "\n",
    "    briers_grid.append(temp_briers_scores_list)\n",
    "    ROC_grid.append(temp_ROC_scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_brier_scores_to_file(briers_grid, era_key='min', event_threshold=event_threshold)\n",
    "save_roc_scores_to_file(ROC_grid, era_key='min', prob_thresh=probability_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_threshold = 500\n",
    "probability_threshold = 0.5\n",
    "ensemble_size = 100\n",
    "\n",
    "briers_grids = [read_brier_scores_file(key, event_threshold) for key in ['min']]\n",
    "ROC_grids = [read_ROC_scores_file(key, probability_threshold) for key in ['min']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, sharey = True)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "lat_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "long_dev_list = np.linspace(0,40,len(lat_dev_list))\n",
    "\n",
    "im1 = axes[0].imshow(briers_grids[0], cmap='inferno', origin = 'lower') \n",
    "im2 = axes[1].imshow(ROC_grids[0], cmap='inferno', origin = 'lower') \n",
    "\n",
    "\n",
    "axes[0].set_yticks(np.arange(len(lat_dev_list))[1::3])\n",
    "axes[0].set_ylabel(r'$\\mathrm{\\sigma_{latitude}}$ [Deg]', fontsize = 'medium')\n",
    "axes[0].set_yticklabels(np.array([str(x) for x in lat_dev_list])[1::3])\n",
    "\n",
    "outer_x_index = [0,1]\n",
    "for x in outer_x_index:\n",
    "    axes[x].set_xticks(np.arange(len(long_dev_list))[1::3])\n",
    "    axes[x].set_xlabel(r'$\\mathrm{\\sigma_{longitude}}$ [Deg]', fontsize = 'medium')\n",
    "    axes[x].set_xticklabels(np.array([f'{x:.0f}' for x in long_dev_list])[1::3], rotation = 0)\n",
    "\n",
    "# Create a single colorbar for all subplots\n",
    "cbar1 = fig.colorbar(im1, ax=axes[0], pad=0.15, orientation = 'horizontal', aspect = 20) #, anchor = (-2,-2), aspect = 30)  \n",
    "cbar1.set_label(f'Briers Score', fontsize = 'medium')\n",
    "\n",
    "cbar2 = fig.colorbar(im2, ax=axes[1], pad=0.15, orientation = 'horizontal', aspect = 20) #, anchor = (-2,-2), aspect = 30)  \n",
    "cbar2.set_label(f'ROC Score', fontsize = 'medium')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fname = 'Briers_ROC_grids_solar_min.svg'\n",
    "path = f'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\{fname}'\n",
    "plt.savefig(path, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_skill_metric = ((1 - briers_grids[0]) + ROC_grids[0])/2\n",
    "\n",
    "lat_dev_list = [0,5,10,15,20,25,30,35,40]\n",
    "long_dev_list = np.linspace(0,40,len(lat_dev_list))\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "im = ax.imshow(combined_skill_metric, cmap='inferno', origin = 'lower') \n",
    "\n",
    "ax.set_yticks(np.arange(len(lat_dev_list))[1::3])\n",
    "ax.set_ylabel(r'$\\mathrm{\\sigma_{latitude}}$ [Deg]')\n",
    "ax.set_yticklabels(np.array([str(x) for x in lat_dev_list])[1::3])\n",
    "\n",
    "ax.set_xticks(np.arange(len(long_dev_list))[1::3])\n",
    "ax.set_xlabel(r'$\\mathrm{\\sigma_{longitude}}$ [Deg]')\n",
    "ax.set_xticklabels(np.array([f'{x:.0f}' for x in long_dev_list])[1::3], rotation = 0)\n",
    "\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.set_label('Combined Brier-ROC Metric')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to store \n",
    "scores_within_bounds_BS = []\n",
    "scores_out_bounds_BS = []\n",
    "\n",
    "scores_within_bounds_ROC = []\n",
    "scores_out_bounds_ROC = []\n",
    "\n",
    "# Get rank histogram/briers/rank grids\n",
    "rank_hist = rank_histogram_grids[0]\n",
    "brier_grid = briers_grids[0]\n",
    "ROC_grid = ROC_grids[0]\n",
    "CR_chunk = solar_era_dict['min']\n",
    "\n",
    "## Briers Score Distribution Separation\n",
    "grid_range = np.max(np.log10(rank_hist)) - np.min(np.log10(rank_hist))\n",
    "within_percentile_of_min = np.log10(rank_hist) < np.min(np.log10(rank_hist)) + grid_range*0.1\n",
    "mindex = np.argwhere(np.log10(rank_hist)== np.min(np.log10(rank_hist)))[0]\n",
    "\n",
    "scores_within_bounds_BS.append(brier_grid[within_percentile_of_min])\n",
    "scores_out_bounds_BS.append(brier_grid[~within_percentile_of_min])\n",
    "\n",
    "scores_within_bounds_BS = np.concatenate(scores_within_bounds_BS)\n",
    "scores_out_bounds_BS = np.concatenate(scores_out_bounds_BS)\n",
    "\n",
    "## ROC Score distribution separation\n",
    "grid_range = np.max(np.log10(rank_hist)) - np.min(np.log10(rank_hist))\n",
    "within_percentile_of_min = np.log10(rank_hist) < np.min(np.log10(rank_hist)) + grid_range*0.1\n",
    "mindex = np.argwhere(np.log10(rank_hist)== np.min(np.log10(rank_hist)))[0]\n",
    "\n",
    "scores_within_bounds_ROC.append(ROC_grid[within_percentile_of_min])\n",
    "scores_out_bounds_ROC.append(ROC_grid[~within_percentile_of_min])\n",
    "\n",
    "scores_within_bounds_ROC = np.concatenate(scores_within_bounds_ROC)\n",
    "scores_out_bounds_ROC = np.concatenate(scores_out_bounds_ROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.12,0.25,15)\n",
    "\n",
    "plt.hist(scores_within_bounds_BS, label = r'Scores within 5% of $\\mathrm{\\log_{10}{\\chi^2}}$ minima', \n",
    "         weights=np.ones(len(scores_within_bounds_BS))/len(scores_within_bounds_BS), bins = bins, color = 'orange')\n",
    "plt.hist(scores_out_bounds_BS, label = 'All other scores', weights=np.ones(len(scores_out_bounds_BS))/len(scores_out_bounds_BS), \n",
    "         bins = bins, fill = False, edgecolor = 'darkblue', hatch = '///')\n",
    "\n",
    "plt.legend(frameon = False, fontsize = 'small')\n",
    "plt.xlabel(f'Briers Score [$\\mathrm{{V_{{threshhold}} = {event_threshold}kms^{{-1}}}}$]')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, 0.4)\n",
    "\n",
    "fname = 'Briers_HIST_solar_min.svg'\n",
    "path = f'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\{fname}'\n",
    "plt.savefig(path, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### Computing CDF for briers scores within/without briers minima bounds\n",
    "hist_1, bin_edges_1 = np.histogram(scores_within_bounds_BS, bins = bins, normed = True)\n",
    "dx1 = bin_edges_1[1] - bin_edges_1[0]\n",
    "F1 = np.cumsum(hist_1)*dx1\n",
    "\n",
    "hist_2, bin_edges_2 = np.histogram(scores_out_bounds_BS, bins = bins, normed = True)\n",
    "dx2 = bin_edges_2[1] - bin_edges_2[0]\n",
    "F2 = np.cumsum(hist_2)*dx2\n",
    "\n",
    "hist_tot, bin_edges_tot = np.histogram(briers_grids[0], bins = bins, normed = True)\n",
    "dx_tot = bin_edges_tot[1] - bin_edges_tot[0]\n",
    "Ftot = np.cumsum(hist_tot)*dx_tot\n",
    "\n",
    "plt.plot(bin_edges_1[1:], F1, marker = 'o', markersize = 2, label = r'CDF of scores within 5% of $\\mathrm{\\log_{10}{\\chi^2}}$ minima', color = 'coral')\n",
    "plt.plot(bin_edges_2[1:], F2, marker = 'o', markersize = 2, label = r'CDF of all other scores', color = 'darkblue')\n",
    "plt.legend(fontsize = 'small')\n",
    "plt.xlabel(f'Briers Score [$\\mathrm{{V_{{threshhold}} = {event_threshold}kms^{{-1}}}}$]')\n",
    "plt.ylabel('Fraction')\n",
    "\n",
    "fname = 'Briers_CDF_solar_min.svg'\n",
    "path = f'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\{fname}'\n",
    "plt.savefig(path, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.6,0.72,15)\n",
    "plt.hist(scores_within_bounds_ROC, label = 'Scores inside rank percentile', \n",
    "         weights=np.ones(len(scores_within_bounds_ROC))/len(scores_within_bounds_ROC), bins = bins, color = 'orange')\n",
    "plt.hist(scores_out_bounds_ROC, label = 'Scores oustide rank percentile', \n",
    "         weights=np.ones(len(scores_out_bounds_ROC))/len(scores_out_bounds_ROC), bins = bins, fill = False, edgecolor = 'darkblue', hatch = '///')\n",
    "plt.legend(frameon = False, fontsize = 'small')\n",
    "plt.xlabel(f'ROC Score [$\\mathrm{{P_{{threshold}}}}$={probability_threshold}]')\n",
    "plt.ylabel('Fraction')\n",
    "\n",
    "fname = 'ROC_HIST_solar_min.svg'\n",
    "path = f'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\{fname}'\n",
    "plt.savefig(path, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### Computing CDF for briers scores within/without briers minima bounds\n",
    "hist_1, bin_edges_1 = np.histogram(scores_within_bounds_ROC, bins = bins, normed = True)\n",
    "dx = bin_edges_1[1] - bin_edges_1[0]\n",
    "F1 = np.cumsum(hist_1)*dx\n",
    "\n",
    "hist_2, bin_edges_2 = np.histogram(scores_out_bounds_ROC, bins = bins, normed = True)\n",
    "dx = bin_edges_2[1] - bin_edges_2[0]\n",
    "F2 = np.cumsum(hist_2)*dx\n",
    "\n",
    "plt.plot(bin_edges_1[1:], F1, marker = 'o', markersize = 2, label = r'CDF of scores within 10% of $\\mathrm{\\log_{10}{\\chi^2}}$ minima', color = 'coral')\n",
    "plt.plot(bin_edges_2[1:], F2, marker = 'o', markersize = 2, label = r'CDF of all other scores', color = 'darkblue')\n",
    "plt.legend(fontsize = 'small')\n",
    "plt.xlabel(f'ROC Score [$\\mathrm{{P_{{threshold}}}}$={probability_threshold}]')\n",
    "plt.ylabel('Fraction')\n",
    "\n",
    "fname = 'ROC_CDF_solar_min.svg'\n",
    "path = f'C:\\\\Users\\\\ct832900\\\\OneDrive - University of Reading\\\\Ensemble Calibration Paper\\\\Figures_01\\\\{fname}'\n",
    "plt.savefig(path, format = 'svg', dpi = 300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recurrence(CR_chunk, omni_data):\n",
    "\n",
    "    forecast_window = 27.2753*u.day\n",
    "    data_chunks = [] # List to store data chunk associated with each CR\n",
    "\n",
    "    # Read in ensemble members per CR\n",
    "    for CR in CR_chunk:\n",
    "\n",
    "        CR_start = pd.Timestamp(carrington_rotation_time(CR).to_datetime()).round('60min')\n",
    "        CR_end = CR_start + pd.Timedelta(forecast_window.value, unit = 'days')\n",
    "        data_chunks.append(omni_data.loc[CR_start:CR_end])\n",
    "\n",
    "    # combining data frame will be useful later on\n",
    "    combined_data = pd.concat(data_chunks)\n",
    "\n",
    "    ## Define carrington rotation offset for index\n",
    "    carrington_rotation = pd.Timedelta(27.25, unit = 'days')\n",
    "    omni_data_reccurance = omni_data.dropna(subset = ['V']) # Remove rows with NaN values\n",
    "\n",
    "    # Isolate data within forecast range + backward additional month needed for 1st carrington rotations of forecast\n",
    "    omni_data_reccurance = omni_data_reccurance.loc[(combined_data.index[0]-carrington_rotation):combined_data.index[-1]]\n",
    "\n",
    "    # Grab index from isolated reccurence data set\n",
    "    omni_data_index = omni_data_reccurance.index\n",
    "\n",
    "    # Project index forward by 1 carrington rotation\n",
    "    reccurance_forecast_index = omni_data_index + carrington_rotation\n",
    "\n",
    "    # Set index for reccurent forecast to the new forward projected datetime series\n",
    "    reccurance_forecast = omni_data_reccurance.set_index(reccurance_forecast_index)\n",
    "    reccurance_forecast = reccurance_forecast.loc[pd.to_datetime(combined_data.index[0]):pd.to_datetime(combined_data.index[-1])]\n",
    "\n",
    "    return reccurance_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'min'\n",
    "\n",
    "CR_list = solar_era_dict[key]\n",
    "\n",
    "CR_sequences = np.split(CR_list, np.where(np.diff(CR_list) > 1)[0] + 1)\n",
    "print(CR_sequences)\n",
    "\n",
    "recurrance_chunks = []\n",
    "for CR_chunk in CR_sequences:\n",
    "    recurrance_chunks.append(generate_recurrence(CR_chunk=CR_chunk, omni_data=omni_data))\n",
    "\n",
    "recurrance_forecast = pd.concat(recurrance_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recurrance_forecast['V'])\n",
    "plt.plot(omni_data['V'])\n",
    "plt.xlim(recurrance_forecast.index[0], recurrance_forecast.index[3000])\n",
    "plt.ylim(200,800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enesmble data parameters\n",
    "key = 'min'\n",
    "lat_dev = 20\n",
    "long_dev = 10\n",
    "ensemble_size = 100\n",
    "event_threshold = 500\n",
    "\n",
    "# Compute calibration curve for best parameters at solar min\n",
    "CR_chunk = solar_era_dict[key]\n",
    "ensemble_members, observed_data = read_in_and_perturb_ensemble(CR_chunk=CR_chunk, lat_dev=lat_dev, long_pert_var=long_dev, \n",
    "                                                                omni_data = omni_data, ensemble_size=ensemble_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in deterministic HUXt output\n",
    "key = 'min'\n",
    "CR_chunk = solar_era_dict[key]\n",
    "\n",
    "deterministic_HUXt_ouput_list = [read_deterministic_forecast_cdf(cr) for cr in CR_chunk]\n",
    "data_chunks = get_data_chunks_per_CR(CR_chunk=CR_chunk, omni_data = omni_data)\n",
    "\n",
    "# Resample deterministic forecast onto omni data timestep\n",
    "resampled_deterministic_output = []\n",
    "for i, df in enumerate(deterministic_HUXt_ouput_list):\n",
    "\n",
    "    df_ = df.set_index('datetime')\n",
    "    resampled_forecast = eef.interpolate_and_resample(observed_data_index = data_chunks[i].index, forecast_index=df_.index, forecast=df_['vsw'])\n",
    "    resampled_deterministic_output.append(resampled_forecast)\n",
    "\n",
    "# Create single timeseries dataframe with datetime index from omni \n",
    "deterministic_forecast = np.concatenate(resampled_deterministic_output)\n",
    "deterministic_forecast = pd.DataFrame({'vsw': deterministic_forecast}, index = observed_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_threshold = 700\n",
    "probability_threshold = 0.5\n",
    "\n",
    "probabilistic_forecast = eef.gen_probabilistic_forecast(ensemble_members=ensemble_members, threshold=action_threshold, ensemble_size=ensemble_size)\n",
    "action_threshold_data = observed_data['V'] > action_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_loss_ratio = 0.1\n",
    "\n",
    "perfect_cost = action_threshold_data\n",
    "percect_loss = np.zeros(len(action_threshold_data))\n",
    "perfect_expense = np.sum(perfect_cost)*cost_loss_ratio\n",
    "\n",
    "# climatology = np.mean(action_threshold_data.to_numpy())\n",
    "# climatological_cost =  probabilistic_forecast > climatology\n",
    "# climatological_loss = perfect_cost == (probabilistic_forecast < climatology)\n",
    "# climatological_expense = np.sum(climatological_cost)*cost_loss_ratio + np.sum(climatological_loss)\n",
    "\n",
    "# climatological probability that wind speed exceeds action threshold\n",
    "climatology = np.mean(action_threshold_data.to_numpy())\n",
    "\n",
    "# climatological cost changes if climatological prob is above/below c/l ratio \n",
    "if cost_loss_ratio <= climatology: # always take action\n",
    "\n",
    "    climatological_cost =  np.ones(len(action_threshold_data))\n",
    "    climatological_loss = 0\n",
    "    climatological_expense = np.sum(climatological_cost)*cost_loss_ratio + np.sum(climatological_loss)\n",
    "\n",
    "    #climatological_expense = len(action_threshold_data)*cost_loss_ratio\n",
    "\n",
    "elif cost_loss_ratio > climatology: # never take action\n",
    "\n",
    "    climatological_cost =  0\n",
    "    climatological_loss = perfect_cost\n",
    "    climatological_expense = np.sum(climatological_cost)*cost_loss_ratio + np.sum(climatological_loss)\n",
    "\n",
    "deterministic_forecast_cost = deterministic_forecast['vsw'] > action_threshold\n",
    "deterministic_forecast_loss = action_threshold_data == (deterministic_forecast['vsw'] < action_threshold)\n",
    "deterministic_expense = np.sum(deterministic_forecast_cost)*cost_loss_ratio + np.sum(deterministic_forecast_loss)\n",
    "\n",
    "ensemble_cost =  probabilistic_forecast > probability_threshold\n",
    "ensemble_loss = action_threshold_data == np.array((probabilistic_forecast < probability_threshold))\n",
    "ensemble_expense = np.sum(ensemble_cost)*cost_loss_ratio + np.sum(ensemble_loss)\n",
    "\n",
    "V_ensemble = 100*(climatological_expense - ensemble_expense)/(climatological_expense - perfect_expense)\n",
    "V_deterministic = 100*(climatological_expense - deterministic_expense)/(climatological_expense - perfect_expense)\n",
    "\n",
    "print(V_ensemble, V_deterministic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_cost_loss(actual_events, forecasted_probabilities, threshold):\n",
    "    \"\"\"\n",
    "    Finding costs and loss based on an action threshold.\n",
    "\n",
    "    Parameters:\n",
    "    actual_events (boolean array): List of actual events (True for event occurred, False otherwise).\n",
    "    forecasted_probabilities (array): List of forecasted probabilities for events.\n",
    "    threshold (float): Threshold for action.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing counts for correct forecasts, incorrect forecasts, and missed events.\n",
    "    \"\"\"\n",
    "    correct_forecasts = 0\n",
    "    incorrect_forecasts = 0\n",
    "    missed_events = 0\n",
    "\n",
    "    for actual, probability in zip(actual_events, forecasted_probabilities):\n",
    "        if actual and probability >= threshold: # correctly forecast event (incurring a cost)\n",
    "            correct_forecasts += 1\n",
    "        elif not actual and probability >= threshold: # incoreectly forecast event (incurring a cost)\n",
    "            incorrect_forecasts += 1\n",
    "        elif actual and probability < threshold: # missing an event (incurring a loss)\n",
    "            missed_events += 1\n",
    "    \n",
    "    return {'correct_costs': correct_forecasts, 'incorrect_costs': incorrect_forecasts, 'loss': missed_events}\n",
    "\n",
    "\n",
    "def calculate_potential_economic_value(actual_events, costs, losses, cost_loss_ratio):\n",
    "\n",
    "    \"\"\"\n",
    "    Finding potenial economic value\n",
    "\n",
    "    Parameters:\n",
    "    actual_events (boolean array): List of actual events (True for event occurred, False otherwise).\n",
    "    costs (float): number of costs\n",
    "    losses (float): number of losses\n",
    "    cost_loss_ratio (float): cost loss ratio for computing potential economic value\n",
    "\n",
    "    Returns:\n",
    "    V: potential economic value\n",
    "    \"\"\"\n",
    "        \n",
    "    climatology_prob = np.mean(actual_events.to_numpy())\n",
    "    perfect_cost = np.sum(actual_events)\n",
    "\n",
    "    # climatological cost changes if climatological prob is above/below c/l ratio \n",
    "    if cost_loss_ratio <= climatology_prob: # always take action\n",
    "\n",
    "        climatological_cost = len(actual_events)\n",
    "        climatological_expense = climatological_cost*cost_loss_ratio\n",
    "    \n",
    "        #climatological_expense = len(action_threshold_data)*cost_loss_ratio\n",
    "\n",
    "    elif cost_loss_ratio > climatology_prob: # never take action\n",
    "\n",
    "        climatological_loss = perfect_cost\n",
    "        climatological_expense = climatological_loss\n",
    "\n",
    "    # forecast expense\n",
    "    forecast_expense = costs*cost_loss_ratio + losses\n",
    "\n",
    "    # potential economic value\n",
    "    V = 100 * (climatological_expense - forecast_expense)/(climatological_expense - perfect_cost*cost_loss_ratio)\n",
    "\n",
    "    return V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_threshold = 500\n",
    "probability_threshold = 0.2\n",
    "probabilistic_forecast = eef.gen_probabilistic_forecast(ensemble_members=ensemble_members, threshold=action_threshold, ensemble_size=ensemble_size)\n",
    "action_threshold_data = observed_data['V'] > action_threshold\n",
    "\n",
    "ensemble_cost_loss_dict = finding_cost_loss(actual_events=action_threshold_data, forecasted_probabilities=probabilistic_forecast, threshold = 0.2)\n",
    "deterministic_cost_loss = finding_cost_loss(actual_events=action_threshold_data, forecasted_probabilities=deterministic_forecast['vsw'] > action_threshold, threshold=1)\n",
    "\n",
    "\n",
    "\n",
    "calculate_potential_economic_value(actual_events=action_threshold_data, costs = ensemble_cost_loss_dict['correct_costs'] + ensemble_cost_loss_dict['incorrect_costs'],\n",
    "                                   losses = ensemble_cost_loss_dict['loss'], cost_loss_ratio = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_loss_ratios = np.linspace(0.001,0.99,50)\n",
    "action_threshold = 500\n",
    "probabilistic_forecast = eef.gen_probabilistic_forecast(ensemble_members=ensemble_members, threshold=action_threshold, ensemble_size=ensemble_size)\n",
    "action_threshold_data = observed_data['V'] > action_threshold\n",
    "\n",
    "PEV_list = []\n",
    "PEV_list_det = []\n",
    "\n",
    "deterministic_cost_loss = finding_cost_loss(actual_events=action_threshold_data, forecasted_probabilities=deterministic_forecast['vsw'] > action_threshold, threshold=1)\n",
    "\n",
    "\n",
    "for cl in cost_loss_ratios:\n",
    "    \n",
    "    prob_thresh = 0.7\n",
    "    #prob_thresh = cl\n",
    "\n",
    "    ensemble_cost_loss_dict = finding_cost_loss(actual_events=action_threshold_data, forecasted_probabilities=probabilistic_forecast, threshold = prob_thresh)\n",
    "    PEV_list.append(calculate_potential_economic_value(actual_events=action_threshold_data, costs = ensemble_cost_loss_dict['correct_costs'] + ensemble_cost_loss_dict['incorrect_costs'],\n",
    "                                   losses = ensemble_cost_loss_dict['loss'], cost_loss_ratio = cl))\n",
    "    \n",
    "    PEV_list_det.append(calculate_potential_economic_value(actual_events=action_threshold_data, costs = deterministic_cost_loss['correct_costs'] + deterministic_cost_loss['incorrect_costs'],\n",
    "                                   losses = deterministic_cost_loss['loss'], cost_loss_ratio = cl))\n",
    "\n",
    "\n",
    "plt.plot(cost_loss_ratios, PEV_list, label = 'Probabilistic', color = 'red')\n",
    "plt.plot(cost_loss_ratios, PEV_list_det, label = 'Deterministic', color = 'blue')\n",
    "\n",
    "\n",
    "plt.xlabel('C/L Ratio')\n",
    "plt.ylabel('Potential Economic Value (%)')\n",
    "plt.legend(frameon = False)\n",
    "plt.axhline(0, color = 'black')\n",
    "plt.ylim(-100,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_window = 27.2753*u.day\n",
    "CR = 2081\n",
    "CR_start = pd.Timestamp(carrington_rotation_time(CR).to_datetime()).round('60min')\n",
    "CR_end = CR_start + pd.Timedelta((forecast_window*2).value, unit = 'days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=1, nrows=2, figsize = (15,5), dpi = 300, sharex = True)\n",
    "\n",
    "axes[0].plot(observed_data['V'], lw = 1, color = 'grey', label = 'OMNI')\n",
    "axes[0].axhline(action_threshold, lw = 1, linestyle = '--', color = 'black', label = 'Action threshold')\n",
    "axes[0].plot(deterministic_forecast['vsw'], color = 'blue', label = 'Deterministic')\n",
    "\n",
    "\n",
    "axes[1].bar(action_threshold_data.index, action_threshold_data.to_numpy(), color = 'grey', label = 'OMNI data (above action threshold)')\n",
    "axes[1].plot(observed_data.index, probabilistic_forecast, color = 'red', label = 'Probabilistic')\n",
    "axes[1].axhline(probability_threshold, linestyle = '--', lw = 1, color = 'black', label = 'Probability action threshold')\n",
    "axes[1].axhline(np.mean(action_threshold_data.to_numpy()), lw = 1, color = 'purple', label = 'Climatology')\n",
    "axes[0].legend(frameon = False, fontsize = 'small')\n",
    "axes[1].legend(frameon = False, fontsize = 'small')\n",
    "\n",
    "plt.xlim(CR_start, CR_end)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Perfect Cost = {np.sum(perfect_cost)}C')\n",
    "print(f'Climatological Cost/Loss = {np.sum(climatological_cost)}C + {np.sum(climatological_loss)}L')\n",
    "print(f'Deterministic Cost/Loss = {np.sum(deterministic_forecast_cost)}C + {np.sum(deterministic_forecast_loss)}L')\n",
    "print(f'Ensemble Cost/Loss = {np.sum(ensemble_cost)}C + {np.sum(ensemble_loss)}L')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HUXT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
